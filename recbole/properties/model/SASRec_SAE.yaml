n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 64                 # (int) The number of features in the hidden state.
inner_size: 256                 # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability. 
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
loss_type: 'BPR'                 # (str) The type of loss function. Range in ['BPR', 'CE'].
train_neg_sample_args: {'distribution': 'uniform', 'sample_num': 1}


input_dim: 64
metrics: ["Recall","MRR","NDCG","Hit","Precision","SAE_Loss_i", "SAE_Loss_u", "SAE_Loss_total", "Deep_LT_Coverage", "GiniIndex", "AveragePopularity", "ItemCoverage"]  # (list or str) Evaluation metrics.


analyze: False
sae_k: [8, 8]
sae_scale_size: [32, 32]
learning_rate:  1e-3
valid_metric: SAE_Loss_total
N: [4096, 4096]
alpha: [0.0, 0.0]
steer: [0, 0]